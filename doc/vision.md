# Техническое видение проекта "LLM-ассистент в Telegram"

## 1. Технологии

- **Язык программирования**: Python
- **Библиотека для Telegram-бота**: `python-telegram-bot`
- **Взаимодействие с LLM**: `OpenRouter` через `openai` клиент.
- **Управление зависимостями**: `pip` и `requirements.txt`.

## 2. Принцип разработки

- **Итеративность**: Начинаем с MVP и постепенно расширяем функционал.
- **Простота (KISS)**: Пишем простой и понятный код, избегаем оверинжиниринга.
- **Без преждевременной оптимизации**: Оптимизируем только по необходимости.

## 3. Структура проекта

```
/
├── .gitignore
├── config.py           # Конфигурация (токены, настройки)
├── requirements.txt    # Зависимости
├── main.py             # Основной файл для запуска бота
└── modules/
    ├── llm.py          # Логика взаимодействия с LLM
    └── bot.py          # Логика работы Telegram-бота (обработчики команд и сообщений)
```

## 4. Архитектура проекта

Проект будет реализован в виде простого **монолитного приложения**.

1.  **Пользователь** отправляет сообщение в Telegram.
2.  `python-telegram-bot` (`bot.py`) получает и обрабатывает сообщение.
3.  При необходимости `bot.py` обращается к `llm.py` для получения ответа от LLM.
4.  `llm.py` отправляет запрос к **OpenRouter API** и возвращает ответ.
5.  `bot.py` отправляет полученный ответ пользователю.

Вся логика выполняется последовательно в рамках одного процесса. Базы данных, очереди и другие внешние компоненты на этапе MVP не используются.

## 5. Модель данных

Для MVP мы избегаем использования баз данных и постоянного хранения.

- **История диалога**: История переписки будет храниться в оперативной памяти во время работы приложения. Мы будем использовать стандартный Python `dict`, где ключом является `ID чата` в Telegram, а значением - `list` сообщений этого диалога.
- **Потеря данных**: При перезапуске бота вся история диалогов будет теряться.

## 6. Работа с LLM

1.  **Системный промпт**: Будет определен как константа в коде. Он будет содержать всю необходимую информацию для работы ассистента: описание услуг, инструкции по общению, ограничения.
2.  **Контекст диалога**: В запрос к LLM будем включать системный промпт и **20 последних сообщений** из истории диалога для сохранения контекста.
3.  **Выбор модели**: Конкретная модель LLM через OpenRouter будет выбрана позже, на этапе реализации.

## 7. Мониторинг LLM

Для MVP используется простое логирование:

- **Логирование запросов**: Все запросы к OpenRouter API будут выводиться в консоль (stdout).
- **Логирование ответов**: Все ответы от LLM также будут логироваться.
- **Анализ качества**: Ручной просмотр логов для оценки качества ответов и корректировки системного промпта.

Автоматизированные системы мониторинга и дашборды добавляются позже по необходимости.

## 8. Сценарии работы

Основные потоки взаимодействия с пользователем:

1.  **Приветствие**: Команда `/start` — бот представляется, объясняет возможности и предлагает задать вопрос.
2.  **Основной диалог**: Пользователь задает вопросы → бот отвечает на основе системного промпта и контекста.
3.  **Призыв к действию**: При заинтересованности пользователя бот предлагает связаться с менеджером.
4.  **Эскалация**: При невозможности ответить бот честно сообщает об ограничениях и предлагает человеческую поддержку.

Все сценарии реализуются через LLM без жесткой логики. Модель сама определяет контекст диалога и выбирает подходящий сценарий.

## 9. Деплой

Деплой осуществляется с использованием **Docker**:

1.  **Локальная разработка**: Запуск через `python main.py` или `docker-compose up`.
2.  **Продакшн**: Сборка Docker-образа и запуск контейнера на VPS.
3.  **Контейнеризация**: Простой `Dockerfile` с Python runtime и зависимостями.
4.  **Управление**: Использование `docker-compose` для упрощения развертывания.

## 10. Подход к конфигурированию

Конфигурация осуществляется через **переменные окружения**:

1.  **Секреты**: `TELEGRAM_BOT_TOKEN`, `OPENROUTER_API_KEY` — передаются через environment variables.
2.  **Настройки приложения**: Модель LLM, лимиты, таймауты — также через env vars с значениями по умолчанию.
3.  **Локальная разработка**: Файл `.env` для удобства (добавляется в `.gitignore`).
4.  **Продакшн**: Переменные окружения передаются в Docker контейнер.

## 11. Подход к логгированию

Используется стандартный модуль `logging` Python:

1.  **Уровни логирования**: 
    - `INFO` — основные события (получено/отправлено сообщение)
    - `DEBUG` — детальная отладка (запросы/ответы LLM)
    - `ERROR` — ошибки API и исключения
2.  **Вывод**: В stdout (консоль), совместимо с Docker
3.  **Формат**: Временная метка, уровень, сообщение
4.  **Конфигурация**: Уровень управляется через `LOG_LEVEL` env var

## 12. Тесты

Минимальное, но достаточное тестирование с использованием `pytest`:

- **Unit-тесты**: Покрытие критичных функций (LLM API, обработка сообщений)
- **Интеграционные тесты**: Базовая проверка работы бота (получение/отправка сообщений)
- **Подход**: Тестируем основную логику, не гонимся за 100% покрытием

## 13. Окружения

Управление зависимостями и окружениями через `uv`:

- **Development**: Локальная разработка с `uv` и `.env` файлом
- **Production**: Docker с зависимостями, установленными через `uv`
- **Изоляция**: Виртуальные окружения для чистоты зависимостей

## 14. Автоматизация

Автоматизация задач через `Makefile`:

- **Команды разработки**: `make install`, `make test`, `make lint`, `make run`
- **Деплой**: `make build`, `make deploy` для Docker операций
- **Простота**: Стандартизированные команды для всех операций
